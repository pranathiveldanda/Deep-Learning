{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "VGhsORG-Zmx5",
    "outputId": "152757a8-51b6-43f0-c66c-590e4962f083"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "dataset_url = 'https://archive.ics.uci.edu/static/public/908/realwaste.zip'\n",
    "download_dir = '/content/downloaded_dataset'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "zip_file_path = os.path.join(download_dir, 'dataset.zip')\n",
    "download_file(dataset_url, zip_file_path)\n",
    "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "8KOzoyABZ2OK",
    "outputId": "a841ca57-0d1c-4504-c768-1a5f8643ea84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Cardboard\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Food Organics\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Glass\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Metal\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Miscellaneous Trash\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Paper\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Plastic\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Textile Trash\n",
      "Augmenting images in /content/downloaded_dataset/realwaste-main/RealWaste\\Vegetation\n",
      "Data augmentation and zip file creation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def random_patch(img, patch_size=(50, 50), random_probability=0.5):\n",
    "    if np.random.rand() < random_probability:\n",
    "        img_width, img_height = img.shape[1], img.shape[0]\n",
    "        patch_x = np.random.randint(0, img_width - patch_size[1] + 1)\n",
    "        patch_y = np.random.randint(0, img_height - patch_size[0] + 1)\n",
    "        patch = img[patch_y:patch_y + patch_size[0], patch_x:patch_x + patch_size[1]]\n",
    "        img_aug = img.copy()\n",
    "        img_aug[patch_y:patch_y + patch_size[0], patch_x:patch_x + patch_size[1]] = np.random.randint(0, 256, patch_size + (img.shape[2],))\n",
    "        return img_aug\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "# Function to perform data augmentation in batches\n",
    "def augment_images_in_batches(dataset_dir, save_dir, batch_size=100, num_augmentations=2):\n",
    "    rotation_range = np.random.randint(0, 180)  \n",
    "    width_shift_range = np.random.uniform(0.0, 0.4)  \n",
    "    height_shift_range = np.random.uniform(0.0, 0.4) \n",
    "    shear_range = np.random.uniform(0.0, 0.4)  \n",
    "    zoom_range = np.random.uniform(0.8, 1.2) \n",
    "\n",
    "    horizontal_flip = np.random.choice([True, False])\n",
    "    fill_mode = 'nearest'\n",
    "    random_patch_probability = 0.5\n",
    "    patch_size = (100, 100) \n",
    "\n",
    "    # Create an ImageDataGenerator with the randomly chosen augmentation settings\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=rotation_range,\n",
    "        width_shift_range=width_shift_range,\n",
    "        height_shift_range=height_shift_range,\n",
    "        shear_range=shear_range,\n",
    "        zoom_range=zoom_range,\n",
    "        horizontal_flip=horizontal_flip,\n",
    "        fill_mode=fill_mode,\n",
    "        preprocessing_function=lambda x: random_patch(x, patch_size=patch_size, random_probability=random_patch_probability)\n",
    "    )\n",
    "\n",
    "    subdirectories = [os.path.join(dataset_dir, d) for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "\n",
    "    for subdir in subdirectories:\n",
    "        print(f\"Augmenting images in {subdir}\")\n",
    "        save_subdir = os.path.join(save_dir, os.path.basename(subdir))\n",
    "        if not os.path.exists(save_subdir):\n",
    "            os.makedirs(save_subdir)\n",
    "        image_files = [os.path.join(subdir, f) for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f))]\n",
    "        num_batches = len(image_files) // batch_size\n",
    "        for batch_index in range(num_batches):\n",
    "            batch_files = image_files[batch_index * batch_size: (batch_index + 1) * batch_size]\n",
    "            for image_file in batch_files:\n",
    "                img = Image.open(image_file)  \n",
    "                x = img.convert('RGB')  \n",
    "                x = np.array(x)  \n",
    "                x = x.reshape((1,) + x.shape) \n",
    "                i = 0\n",
    "                for batch in datagen.flow(x, batch_size=1, save_to_dir=save_subdir, save_prefix='aug', save_format='jpg'):\n",
    "                    i += 1\n",
    "                    if i > num_augmentations:  \n",
    "                        break  \n",
    "\n",
    "\n",
    "dataset_dir = '/content/downloaded_dataset/realwaste-main/RealWaste'\n",
    "save_dir = '/content/augmented_dataset'\n",
    "augment_images_in_batches(dataset_dir, save_dir)\n",
    "\n",
    "with ZipFile('/content/augmented_dataset2.zip', 'w') as zipf:\n",
    "    for root, dirs, files in os.walk(save_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), save_dir))\n",
    "\n",
    "print(\"Data augmentation and zip file creation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsK5yGDNGx8x",
    "outputId": "cd2db4af-862f-404f-865e-47b60c39b1a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "Directory: /content/augmented_dataset/Cardboard, Total Samples: 1137\n",
      "Directory: /content/augmented_dataset/Food Organics, Total Samples: 1133\n",
      "Directory: /content/augmented_dataset/Glass, Total Samples: 1119\n",
      "Directory: /content/augmented_dataset/Metal, Total Samples: 1891\n",
      "Directory: /content/augmented_dataset/Miscellaneous Trash, Total Samples: 1121\n",
      "Directory: /content/augmented_dataset/Paper, Total Samples: 1389\n",
      "Directory: /content/augmented_dataset/Plastic, Total Samples: 2361\n",
      "Directory: /content/augmented_dataset/Textile Trash, Total Samples: 865\n",
      "Directory: /content/augmented_dataset/Vegetation, Total Samples: 1112\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to generate a report containing the number of total samples in each directory\n",
    "def generate_report(dataset_dir):\n",
    "    # Get subdirectories in the dataset directory\n",
    "    subdirectories = [os.path.join(dataset_dir, d) for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "\n",
    "    print(\"Report:\")\n",
    "    for subdir in subdirectories:\n",
    "        # Count the number of image files in the subdirectory\n",
    "        num_samples = len([f for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f))])\n",
    "\n",
    "        print(f\"Directory: {subdir}, Total Samples: {num_samples}\")\n",
    "\n",
    "# Define the directory where the dataset is located\n",
    "dataset_dir = '/content/augmented_dataset/'\n",
    "\n",
    "# Generate and print the report\n",
    "generate_report(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uxgjzgLlPTYI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.220\n",
      "[1,    40] loss: 0.219\n",
      "[1,    60] loss: 0.218\n",
      "[1,    80] loss: 0.218\n",
      "[2,    20] loss: 0.216\n",
      "[2,    40] loss: 0.217\n",
      "[2,    60] loss: 0.215\n",
      "[2,    80] loss: 0.215\n",
      "[3,    20] loss: 0.213\n",
      "[3,    40] loss: 0.215\n",
      "[3,    60] loss: 0.214\n",
      "[3,    80] loss: 0.213\n",
      "[4,    20] loss: 0.212\n",
      "[4,    40] loss: 0.215\n",
      "[4,    60] loss: 0.213\n",
      "[4,    80] loss: 0.212\n",
      "[5,    20] loss: 0.210\n",
      "[5,    40] loss: 0.214\n",
      "[5,    60] loss: 0.211\n",
      "[5,    80] loss: 0.210\n",
      "[6,    20] loss: 0.208\n",
      "[6,    40] loss: 0.211\n",
      "[6,    60] loss: 0.208\n",
      "[6,    80] loss: 0.207\n",
      "[7,    20] loss: 0.205\n",
      "[7,    40] loss: 0.207\n",
      "[7,    60] loss: 0.203\n",
      "[7,    80] loss: 0.201\n",
      "[8,    20] loss: 0.200\n",
      "[8,    40] loss: 0.202\n",
      "[8,    60] loss: 0.197\n",
      "[8,    80] loss: 0.196\n",
      "[9,    20] loss: 0.195\n",
      "[9,    40] loss: 0.198\n",
      "[9,    60] loss: 0.192\n",
      "[9,    80] loss: 0.190\n",
      "[10,    20] loss: 0.191\n",
      "[10,    40] loss: 0.193\n",
      "[10,    60] loss: 0.187\n",
      "[10,    80] loss: 0.185\n",
      "Finished Training\n",
      "Accuracy of the network for 1188 images: 33 %\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  \n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "])\n",
    "dataset = torchvision.datasets.ImageFolder(root='/content/downloaded_dataset/realwaste-main/RealWaste', transform=transform)\n",
    "trainset, testset = train_test_split(dataset, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=36, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=36, shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network for %d images: %d %%' % (total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.219\n",
      "[1,    40] loss: 0.219\n",
      "[1,    60] loss: 0.219\n",
      "[1,    80] loss: 0.218\n",
      "[1,   100] loss: 0.218\n",
      "[1,   120] loss: 0.217\n",
      "[1,   140] loss: 0.217\n",
      "[1,   160] loss: 0.217\n",
      "[1,   180] loss: 0.217\n",
      "[1,   200] loss: 0.216\n",
      "[1,   220] loss: 0.215\n",
      "[1,   240] loss: 0.217\n",
      "[2,    20] loss: 0.216\n",
      "[2,    40] loss: 0.216\n",
      "[2,    60] loss: 0.216\n",
      "[2,    80] loss: 0.216\n",
      "[2,   100] loss: 0.214\n",
      "[2,   120] loss: 0.212\n",
      "[2,   140] loss: 0.214\n",
      "[2,   160] loss: 0.215\n",
      "[2,   180] loss: 0.215\n",
      "[2,   200] loss: 0.214\n",
      "[2,   220] loss: 0.213\n",
      "[2,   240] loss: 0.216\n",
      "[3,    20] loss: 0.214\n",
      "[3,    40] loss: 0.215\n",
      "[3,    60] loss: 0.215\n",
      "[3,    80] loss: 0.215\n",
      "[3,   100] loss: 0.213\n",
      "[3,   120] loss: 0.210\n",
      "[3,   140] loss: 0.213\n",
      "[3,   160] loss: 0.214\n",
      "[3,   180] loss: 0.213\n",
      "[3,   200] loss: 0.212\n",
      "[3,   220] loss: 0.212\n",
      "[3,   240] loss: 0.214\n",
      "[4,    20] loss: 0.212\n",
      "[4,    40] loss: 0.212\n",
      "[4,    60] loss: 0.213\n",
      "[4,    80] loss: 0.212\n",
      "[4,   100] loss: 0.210\n",
      "[4,   120] loss: 0.207\n",
      "[4,   140] loss: 0.210\n",
      "[4,   160] loss: 0.210\n",
      "[4,   180] loss: 0.209\n",
      "[4,   200] loss: 0.208\n",
      "[4,   220] loss: 0.207\n",
      "[4,   240] loss: 0.208\n",
      "[5,    20] loss: 0.205\n",
      "[5,    40] loss: 0.204\n",
      "[5,    60] loss: 0.206\n",
      "[5,    80] loss: 0.203\n",
      "[5,   100] loss: 0.203\n",
      "[5,   120] loss: 0.197\n",
      "[5,   140] loss: 0.203\n",
      "[5,   160] loss: 0.201\n",
      "[5,   180] loss: 0.198\n",
      "[5,   200] loss: 0.199\n",
      "[5,   220] loss: 0.198\n",
      "[5,   240] loss: 0.199\n",
      "[6,    20] loss: 0.197\n",
      "[6,    40] loss: 0.195\n",
      "[6,    60] loss: 0.199\n",
      "[6,    80] loss: 0.194\n",
      "[6,   100] loss: 0.195\n",
      "[6,   120] loss: 0.190\n",
      "[6,   140] loss: 0.197\n",
      "[6,   160] loss: 0.194\n",
      "[6,   180] loss: 0.188\n",
      "[6,   200] loss: 0.190\n",
      "[6,   220] loss: 0.190\n",
      "[6,   240] loss: 0.193\n",
      "[7,    20] loss: 0.190\n",
      "[7,    40] loss: 0.189\n",
      "[7,    60] loss: 0.192\n",
      "[7,    80] loss: 0.188\n",
      "[7,   100] loss: 0.189\n",
      "[7,   120] loss: 0.184\n",
      "[7,   140] loss: 0.191\n",
      "[7,   160] loss: 0.188\n",
      "[7,   180] loss: 0.181\n",
      "[7,   200] loss: 0.184\n",
      "[7,   220] loss: 0.183\n",
      "[7,   240] loss: 0.188\n",
      "[8,    20] loss: 0.186\n",
      "[8,    40] loss: 0.183\n",
      "[8,    60] loss: 0.187\n",
      "[8,    80] loss: 0.184\n",
      "[8,   100] loss: 0.183\n",
      "[8,   120] loss: 0.178\n",
      "[8,   140] loss: 0.186\n",
      "[8,   160] loss: 0.184\n",
      "[8,   180] loss: 0.175\n",
      "[8,   200] loss: 0.178\n",
      "[8,   220] loss: 0.178\n",
      "[8,   240] loss: 0.183\n",
      "[9,    20] loss: 0.181\n",
      "[9,    40] loss: 0.178\n",
      "[9,    60] loss: 0.183\n",
      "[9,    80] loss: 0.180\n",
      "[9,   100] loss: 0.178\n",
      "[9,   120] loss: 0.173\n",
      "[9,   140] loss: 0.181\n",
      "[9,   160] loss: 0.179\n",
      "[9,   180] loss: 0.170\n",
      "[9,   200] loss: 0.173\n",
      "[9,   220] loss: 0.173\n",
      "[9,   240] loss: 0.178\n",
      "[10,    20] loss: 0.176\n",
      "[10,    40] loss: 0.173\n",
      "[10,    60] loss: 0.178\n",
      "[10,    80] loss: 0.176\n",
      "[10,   100] loss: 0.172\n",
      "[10,   120] loss: 0.168\n",
      "[10,   140] loss: 0.176\n",
      "[10,   160] loss: 0.174\n",
      "[10,   180] loss: 0.165\n",
      "[10,   200] loss: 0.167\n",
      "[10,   220] loss: 0.168\n",
      "[10,   240] loss: 0.173\n",
      "Finished Training\n",
      "Accuracy of the network: 38 %\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  \n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "])\n",
    "dataset = torchvision.datasets.ImageFolder(root='/content/augmented_dataset/', transform=transform)\n",
    "trainset, testset = train_test_split(dataset, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=36, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=36, shuffle=False)\n",
    "\n",
    "class Net_aug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_aug, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net_aug = Net_aug()\n",
    "criterion_aug = nn.CrossEntropyLoss()\n",
    "optimizer_aug = optim.SGD(net_aug.parameters(), lr=0.01)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_lossaug = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_aug.zero_grad()\n",
    "        outputs = net_aug(inputs)\n",
    "        loss_aug = criterion_aug(outputs, labels)\n",
    "        loss_aug.backward()\n",
    "        optimizer_aug.step()\n",
    "        running_lossaug += loss_aug.item()\n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_lossaug / 200))\n",
    "            running_lossaug = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net_aug(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
