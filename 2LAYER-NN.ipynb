{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa494c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.clear_all_output();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.clear_all_output();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8578f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 53 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2000,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2000,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 512) \n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        x_train, y_train = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199: \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct =0\n",
    "total =0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x_test,y_test = data[0], data[1]\n",
    "        outputs = net(x_test)\n",
    "        _, y_pred = torch.max(outputs.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (y_pred == y_test).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad29203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1.0: Loss 2.298655034545292\n",
      "Iteration 2.0: Loss 2.0464697210666705\n",
      "Iteration 3.0: Loss 1.9314129759715715\n",
      "Iteration 4.0: Loss 1.8309072343523494\n",
      "Iteration 5.0: Loss 1.796154947078394\n",
      "Iteration 6.0: Loss 1.7577056495982877\n",
      "Iteration 7.0: Loss 1.7202386985853362\n",
      "Iteration 8.0: Loss 1.6884538591710443\n",
      "Iteration 9.0: Loss 1.6869641304936165\n",
      "Iteration 10.0: Loss 1.6513777917897\n",
      "Accuracy: 43.919999999999995\n"
     ]
    }
   ],
   "source": [
    "def to_numpy(dataset):\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    data = next(iter(data_loader))\n",
    "    images, labels = data\n",
    "    images = images.numpy()\n",
    "    labels = labels.numpy()\n",
    "    images = images.reshape(images.shape[0], -1)\n",
    "    return images, labels\n",
    "x_train, y_train = to_numpy(trainset)\n",
    "x_test, y_test = to_numpy(testset)\n",
    "\n",
    "\n",
    "def encoding(y):\n",
    "    en = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        en[i, y[i]] = 1\n",
    "    return en\n",
    "\n",
    "y_train_encoded = encoding(y_train)\n",
    "y_test_encoded = encoding(y_test)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def softmax(x):\n",
    "    exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    h1 = relu(np.dot(X, W1) + b1)\n",
    "    scores = np.dot(h1, W2) + b2\n",
    "    probs = softmax(scores)\n",
    "    return h1, probs\n",
    "def backward_pass(X, h1, probs, W1, W2, Y):\n",
    "    dist = probs - Y\n",
    "    dW2 = np.dot(h1.T, dist)\n",
    "    db2 = np.sum(dist, axis=0, keepdims=True)\n",
    "    dh1 = np.dot(dist, W2.T)\n",
    "    dh1[h1 <= 0] = 0\n",
    "    dW1 = np.dot(X.T, dh1)\n",
    "    db1 = np.sum(dh1, axis=0, keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "learning_rate = 1e-4\n",
    "itr = 100\n",
    "batch = 2000\n",
    "hidden_layer = 512\n",
    "\n",
    "W1 = 0.01 * np.random.randn(3072, hidden_layer)\n",
    "b1 = np.zeros((1, hidden_layer))\n",
    "W2 = 0.01 * np.random.randn(hidden_layer, 10)\n",
    "b2 = np.zeros((1, 10))\n",
    "\n",
    "\n",
    "for i in range(itr):\n",
    "    idx = np.random.choice(x_train.shape[0], batch, replace=False)\n",
    "    x_batch = x_train[idx]\n",
    "    y_batch = y_train_encoded[idx]\n",
    "    hidden_layer, train_probs = forward_pass(x_batch, W1, b1, W2, b2)\n",
    "    loss = np.mean(-np.log(train_probs[range(batch), y_batch.argmax(axis=1)]))\n",
    "    dW1, db1, dW2, db2 = backward_pass(x_batch, hidden_layer, train_probs, W1, W2, y_batch)\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {(i/10)+1}: Loss {loss}')\n",
    "    \n",
    "\n",
    "_, test_probs = forward_pass(x_test, W1, b1, W2, b2)\n",
    "y_pred = np.argmax(test_probs, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89743dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for lambda = 0.0\n",
      "Iteration 1.0: Loss 1.652690598052167\n",
      "Iteration 2.0: Loss 1.5859499977318872\n",
      "Iteration 3.0: Loss 1.5777135878769477\n",
      "Iteration 4.0: Loss 1.596567333497947\n",
      "Iteration 5.0: Loss 1.5500282197759534\n",
      "Iteration 6.0: Loss 1.578628496971099\n",
      "Iteration 7.0: Loss 1.5386313962173317\n",
      "Iteration 8.0: Loss 1.5332602526332497\n",
      "Iteration 9.0: Loss 1.5071013837999507\n",
      "Iteration 10.0: Loss 1.5221264510599026\n",
      "Loss for lambda = 0.1\n",
      "Iteration 1.0: Loss 2.7896923204449413\n",
      "Iteration 2.0: Loss 2.830667741322787\n",
      "Iteration 3.0: Loss 2.771223094808242\n",
      "Iteration 4.0: Loss 2.782680979705247\n",
      "Iteration 5.0: Loss 2.7893493285668294\n",
      "Iteration 6.0: Loss 2.834108202013612\n",
      "Iteration 7.0: Loss 2.793645708923129\n",
      "Iteration 8.0: Loss 2.774742742842367\n",
      "Iteration 9.0: Loss 2.7884143316079983\n",
      "Iteration 10.0: Loss 2.8093388608111676\n",
      "Loss for lambda = 0.2\n",
      "Iteration 1.0: Loss 4.1644025743717705\n",
      "Iteration 2.0: Loss 4.167835309530966\n",
      "Iteration 3.0: Loss 4.18585826314491\n",
      "Iteration 4.0: Loss 4.166820723742145\n",
      "Iteration 5.0: Loss 4.17145150589192\n",
      "Iteration 6.0: Loss 4.16195990517186\n",
      "Iteration 7.0: Loss 4.142335046928646\n",
      "Iteration 8.0: Loss 4.185168243829791\n",
      "Iteration 9.0: Loss 4.097833555535228\n",
      "Iteration 10.0: Loss 4.1792856982530076\n",
      "Loss for lambda = 0.3\n",
      "Iteration 1.0: Loss 5.521055054154891\n",
      "Iteration 2.0: Loss 5.5258614118684655\n",
      "Iteration 3.0: Loss 5.604346452595033\n",
      "Iteration 4.0: Loss 5.595150289800813\n",
      "Iteration 5.0: Loss 5.564563025752472\n",
      "Iteration 6.0: Loss 5.52227112742009\n",
      "Iteration 7.0: Loss 5.600089618512689\n",
      "Iteration 8.0: Loss 5.549219609455243\n",
      "Iteration 9.0: Loss 5.659097244916348\n",
      "Iteration 10.0: Loss 5.582339306565476\n",
      "Loss for lambda = 0.4\n",
      "Iteration 1.0: Loss 7.088800583318884\n",
      "Iteration 2.0: Loss 7.045541795544969\n",
      "Iteration 3.0: Loss 7.000434388362676\n",
      "Iteration 4.0: Loss 7.014521840310787\n",
      "Iteration 5.0: Loss 7.050745263474939\n",
      "Iteration 6.0: Loss 7.050220286149824\n",
      "Iteration 7.0: Loss 7.048113285768809\n",
      "Iteration 8.0: Loss 7.0644581286017525\n",
      "Iteration 9.0: Loss 7.135249316155063\n",
      "Iteration 10.0: Loss 7.046309110803268\n",
      "Loss for lambda = 0.5\n",
      "Iteration 1.0: Loss 8.54895647086808\n",
      "Iteration 2.0: Loss 8.605015145032562\n",
      "Iteration 3.0: Loss 8.604047020008412\n",
      "Iteration 4.0: Loss 8.547732808633992\n",
      "Iteration 5.0: Loss 8.6388052139832\n",
      "Iteration 6.0: Loss 8.63719236174859\n",
      "Iteration 7.0: Loss 8.619352423703301\n",
      "Iteration 8.0: Loss 8.625899094905769\n",
      "Iteration 9.0: Loss 8.672962033990688\n",
      "Iteration 10.0: Loss 8.660816462446107\n",
      "Loss for lambda = 0.6\n",
      "Iteration 1.0: Loss 10.19485201544314\n",
      "Iteration 2.0: Loss 10.178095211974117\n",
      "Iteration 3.0: Loss 10.142630321145484\n",
      "Iteration 4.0: Loss 10.142272799971543\n",
      "Iteration 5.0: Loss 10.215382459896917\n",
      "Iteration 6.0: Loss 10.213530799472975\n",
      "Iteration 7.0: Loss 10.297748473062452\n",
      "Iteration 8.0: Loss 10.253721505207537\n",
      "Iteration 9.0: Loss 10.40587423689327\n",
      "Iteration 10.0: Loss 10.256274733831837\n",
      "Loss for lambda = 0.7\n",
      "Iteration 1.0: Loss 11.846877173683701\n",
      "Iteration 2.0: Loss 11.889487646382232\n",
      "Iteration 3.0: Loss 12.164269232724603\n",
      "Iteration 4.0: Loss 11.872351336669048\n",
      "Iteration 5.0: Loss 11.9564551159275\n",
      "Iteration 6.0: Loss 11.85160558007183\n",
      "Iteration 7.0: Loss 11.992174421941082\n",
      "Iteration 8.0: Loss 11.867699844996729\n",
      "Iteration 9.0: Loss 11.965351788419758\n",
      "Iteration 10.0: Loss 12.022429645753753\n",
      "Loss for lambda = 0.8\n",
      "Iteration 1.0: Loss 13.471656636315135\n",
      "Iteration 2.0: Loss 13.569732733241551\n",
      "Iteration 3.0: Loss 13.54995132006616\n",
      "Iteration 4.0: Loss 13.684868764385014\n",
      "Iteration 5.0: Loss 13.652660097293197\n",
      "Iteration 6.0: Loss 13.675488846295865\n",
      "Iteration 7.0: Loss 13.643908078740381\n",
      "Iteration 8.0: Loss 13.716681928391923\n",
      "Iteration 9.0: Loss 13.686592384937283\n",
      "Iteration 10.0: Loss 13.712880104250216\n",
      "Loss for lambda = 0.9\n",
      "Iteration 1.0: Loss 15.429679248374239\n",
      "Iteration 2.0: Loss 15.357332090883052\n",
      "Iteration 3.0: Loss 15.498007833415098\n",
      "Iteration 4.0: Loss 15.362523274781354\n",
      "Iteration 5.0: Loss 15.415635150685985\n",
      "Iteration 6.0: Loss 15.403375335258076\n",
      "Iteration 7.0: Loss 15.408064176241693\n",
      "Iteration 8.0: Loss 15.423314184200752\n",
      "Iteration 9.0: Loss 15.587404901323776\n",
      "Iteration 10.0: Loss 15.487866108321006\n",
      "Accuracy: 50.17\n"
     ]
    }
   ],
   "source": [
    "#backward propagation with regularization of loss\n",
    "def regbackward_pass(X, h1, probs, W1, W2, Y, reg_strength):\n",
    "    dist = probs - Y\n",
    "    dW2 = np.dot(h1.T, dist)\n",
    "    db2 = np.sum(dist, axis=0, keepdims=True)\n",
    "    dh1 = np.dot(dist, W2.T)\n",
    "    dh1[h1 <= 0] = 0\n",
    "    dW1 = np.dot(X.T, dh1)\n",
    "    db1 = np.sum(dh1, axis=0, keepdims=True)\n",
    "    dW2 += reg_strength * W2\n",
    "    dW1 += reg_strength * W1\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "for reg_strength in range(10):\n",
    "    print(f'Loss for lambda = {reg_strength/10}')\n",
    "    for i in range(itr):\n",
    "        idx = np.random.choice(x_train.shape[0], batch, replace=False)\n",
    "        x_batch = x_train[idx]\n",
    "        y_batch = y_train_encoded[idx]\n",
    "        hidden_layer, train_probs = forward_pass(x_batch, W1, b1, W2, b2)\n",
    "        loss = np.mean(-np.log(train_probs[range(batch), y_batch.argmax(axis=1)])) + (reg_strength/10) * np.sqrt((np.sum(W1 * W1) + np.sum(W2 * W2)))\n",
    "        dW1, db1, dW2, db2 = regbackward_pass(x_batch, hidden_layer, train_probs, W1, W2, y_batch,reg_strength/10)\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        if i % 10 == 0:\n",
    "            print(f'Iteration {(i/10)+1}: Loss {loss}')\n",
    "    \n",
    "\n",
    "_, test_probs = forward_pass(x_test, W1, b1, W2, b2)\n",
    "y_pred = np.argmax(test_probs, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24dfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
